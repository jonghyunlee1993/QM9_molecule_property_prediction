{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e34c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics.functional as tm\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216ac9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_df = pd.read_csv(\"data/mol_id_to_smiles.csv\")\n",
    "\n",
    "train_data = torch.load(\"data/qm9_train_data.pt\")\n",
    "X_train = train_data['mol_id']\n",
    "X_train = pd.DataFrame(X_train, columns=[\"ID\"]).merge(smiles_df, how=\"left\", left_on=\"ID\", right_on=\"id\")['smiles'].values\n",
    "y_train = train_data['mu']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "test_data = torch.load(\"data/qm9_test_data.pt\")\n",
    "X_test = test_data[\"mol_id\"]\n",
    "X_test = pd.DataFrame(X_test, columns=[\"ID\"]).merge(smiles_df, how=\"left\", left_on=\"ID\", right_on=\"id\")['smiles'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2c09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jonghyunlee/DrugLikeMoleculeBERT\")\n",
    "encoder = AutoModel.from_pretrained(\"jonghyunlee/DrugLikeMoleculeBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6faa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, X, y=None, max_length=128, is_predict=False):\n",
    "        self.X = X\n",
    "        self.is_predict = is_predict\n",
    "\n",
    "        if not self.is_predict:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = None\n",
    "            \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def encode(self, sequence):\n",
    "        return self.tokenizer.encode_plus(\" \".join(sequence), max_length=self.max_length, truncation=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.encode(self.X[idx])\n",
    "        \n",
    "        if not self.is_predict:\n",
    "            y = self.y[idx]\n",
    "            return smiles, y\n",
    "        else:\n",
    "            return smiles\n",
    "    \n",
    "    \n",
    "def collate_batch_train(batch):\n",
    "    smiles, y = [], []\n",
    "    \n",
    "    for (smiles_, y_) in batch:\n",
    "        smiles.append(smiles_)\n",
    "        y.append(y_)\n",
    "        \n",
    "    smiles = tokenizer.pad(smiles, return_tensors=\"pt\")\n",
    "    y = torch.tensor(y).float()\n",
    "    \n",
    "    return smiles, y\n",
    "    \n",
    "    \n",
    "def collate_batch_test(batch):\n",
    "    smiles = []\n",
    "    \n",
    "    for (smiles_) in batch:\n",
    "        smiles.append(smiles_)\n",
    "        \n",
    "    smiles = tokenizer.pad(smiles, return_tensors=\"pt\")\n",
    "    \n",
    "    return smiles\n",
    "\n",
    "\n",
    "train_dataset = SmilesDataset(tokenizer, X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, \n",
    "                              collate_fn=collate_batch_train, \n",
    "                              num_workers=16, pin_memory=True, \n",
    "                              prefetch_factor=10, drop_last=True)\n",
    "\n",
    "valid_dataset = SmilesDataset(tokenizer, X_valid, y_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=256, \n",
    "                              collate_fn=collate_batch_train, \n",
    "                              num_workers=16, pin_memory=True, \n",
    "                              prefetch_factor=10)\n",
    "\n",
    "test_dataset = SmilesDataset(tokenizer, X_test, is_predict=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, \n",
    "                             collate_fn=collate_batch_test, \n",
    "                             num_workers=16, pin_memory=True, \n",
    "                             prefetch_factor=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbd33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, encoder, input_dim=128, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        for param in self.encoder.encoder.layer[0:-1].parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.align = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.Linear(input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, smiles):\n",
    "        x = self.encoder(**smiles)\n",
    "        x = x.pooler_output\n",
    "        x = self.align(x)\n",
    "\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = F.gelu(self.fc3(x))\n",
    "        out = self.fc_out(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e40611ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:429: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "class MoleculePropertyPredictor(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    \n",
    "    def step(self, batch):\n",
    "        smiles, y = batch\n",
    "        pred = self.model(smiles).squeeze(-1)\n",
    "        loss = F.l1_loss(pred, y)\n",
    "        acc = tm.mean_squared_error(pred, y)\n",
    "        \n",
    "        return loss, acc\n",
    "    \n",
    "       \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.step(batch)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self.step(batch)\n",
    "        \n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"valid_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        return self.model(batch)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "    \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "    \n",
    "    \n",
    "model = BERT(encoder)\n",
    "predictor = MoleculePropertyPredictor(model)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(monitor='valid_loss', save_top_k=3, dirpath='weights/BERT', filename='BERT-{epoch:03d}-{valid_loss:.4f}-{valid_acc:.4f}'),\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=500, gpus=1, enable_progress_bar=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2feaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | BERT | 3.8 M \n",
      "-------------------------------\n",
      "2.4 M     Trainable params\n",
      "1.4 M     Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.179    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cf8848579d4c039d2a468134d4a535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(predictor, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_fname = \"\"\n",
    "predictor = predictor.load_from_checkpoint(\"weights/BERT/\" + ckpt_fname, model=model)\n",
    "\n",
    "pred = trainer.predict(predictor, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace4c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "def to_np(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "\n",
    "for p in tqdm(pred):\n",
    "    preds.append(to_np(p))\n",
    "\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "np.savetxt('pred.csv', preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
